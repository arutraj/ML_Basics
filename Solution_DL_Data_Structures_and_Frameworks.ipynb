{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arutraj/ML_Basics/blob/main/Solution_DL_Data_Structures_and_Frameworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: Design the neural network"
      ],
      "metadata": {
        "id": "IeFhLLUhj4Bg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65TY5LNwjydD",
        "outputId": "b194785c-3339-4bde-d7eb-23632275580c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output tensor: tensor([[-0.2675, -0.1715,  0.3112]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Step 1: Layer Dimensions\n",
        "input_dim = 5\n",
        "hidden_dim = 10\n",
        "output_dim = 3\n",
        "\n",
        "# Step 2: Activation Function\n",
        "def activation_function(x):\n",
        "    return torch.relu(x)  # Using ReLU activation function\n",
        "\n",
        "# Step 3: Neural Network Architecture\n",
        "class ThreeLayerNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ThreeLayerNN, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
        "        self.hidden_layer = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hidden_output = activation_function(self.input_layer(x))\n",
        "        hidden_output = activation_function(self.hidden_layer(hidden_output))\n",
        "        output = self.output_layer(hidden_output)\n",
        "        return output\n",
        "\n",
        "# Step 4: Initialization and Testing\n",
        "x = torch.randn(1, input_dim)\n",
        "model = ThreeLayerNN()\n",
        "output = model(x)\n",
        "print(\"Output tensor:\", output)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BV87lFa-47bj",
        "outputId": "1ecc0985-78ba-493c-e353-f88565c0c060",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Autograd"
      ],
      "metadata": {
        "id": "rZSxK961sDfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the real function f(x) = (x^4 - 3x^3 + 2x^2 - 5x + 7) / (2x^3 + 5x^2 - x + 3)\n",
        "def real_function(x):\n",
        "    result = (x**4 - 3*x**3 + 2*x**2 - 5*x + 7) / (2*x**3 + 5*x**2 - x + 3)\n",
        "    return result\n",
        "\n",
        "# Define a real number for which to compute the gradient\n",
        "x = torch.tensor(1.0, requires_grad=True, dtype=torch.float32)\n",
        "\n",
        "# Compute the real function f(x)\n",
        "result = real_function(x)\n",
        "\n",
        "# Compute the gradient of f(x) with respect to x\n",
        "result.backward()\n",
        "\n",
        "# Obtain the gradient of x\n",
        "gradient_x = x.grad\n",
        "\n",
        "print(\"Gradient of x:\", gradient_x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUj-JHOrsHhg",
        "outputId": "e10e7d1c-a205-40ae-b8e3-4353adf1304d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of x: tensor(-1.0370)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional Question: Logistic Regression"
      ],
      "metadata": {
        "id": "Pr-jsQ4ksQNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assignment Question 1:\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "# Define the number of features (input size)\n",
        "input_size = 2\n",
        "\n",
        "# Define the number of classes (output size)\n",
        "output_size = 2\n",
        "\n",
        "# Create the training dataset\n",
        "X_train = torch.tensor([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [5.0, 1.0]])\n",
        "y_train = torch.tensor([0, 0, 1, 1])  # Binary classification labels (0 or 1)\n",
        "\n",
        "# Step 2: Define the logistic regression model\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))\n",
        "\n",
        "model = LogisticRegressionModel()\n",
        "\n",
        "# Step 3: Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss is used for binary classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Step 4: Train the model\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Step 5: Test the model\n",
        "X_test = torch.tensor([[4.0, 3.0]])\n",
        "predicted = model(X_test)\n",
        "predicted_class = torch.argmax(predicted).item()\n",
        "print(\"Predicted class:\", predicted_class)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV1XWQNxsG77",
        "outputId": "1846c68a-5a26-4333-9394-cba234e4a989"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 0.6747\n",
            "Epoch [200/1000], Loss: 0.6543\n",
            "Epoch [300/1000], Loss: 0.6419\n",
            "Epoch [400/1000], Loss: 0.6333\n",
            "Epoch [500/1000], Loss: 0.6265\n",
            "Epoch [600/1000], Loss: 0.6206\n",
            "Epoch [700/1000], Loss: 0.6152\n",
            "Epoch [800/1000], Loss: 0.6101\n",
            "Epoch [900/1000], Loss: 0.6052\n",
            "Epoch [1000/1000], Loss: 0.6006\n",
            "Predicted class: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional Question: Attention with Einsum"
      ],
      "metadata": {
        "id": "WQBRx46zsXx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def single_head_attention_with_einsum(query, key, value):\n",
        "    # Calculate the dot product between query and key using einsum\n",
        "    attention_scores = torch.einsum('bqd,bkd->bqk', query, key)\n",
        "\n",
        "    # Scale the dot product by dividing it by the square root of the dimension of the key vector\n",
        "    scaled_attention_scores = attention_scores / (key.size(-1) ** 0.5)\n",
        "\n",
        "    # Apply softmax to obtain attention weights along the last dimension (key dimension)\n",
        "    attention_weights = F.softmax(scaled_attention_scores, dim=-1)\n",
        "\n",
        "    # Compute the weighted sum of value vectors using einsum\n",
        "    attended_values = torch.einsum('bqk,bvd->bqd', attention_weights, value)\n",
        "\n",
        "    return attended_values, attention_weights\n"
      ],
      "metadata": {
        "id": "WXLTakhRsTwS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DRSk3fjDsd54"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}