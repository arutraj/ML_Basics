{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arutraj/ML_Basics/blob/main/17_9_Word_Embeddings_in_Action_Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWWH8Gvgd09_"
      },
      "source": [
        "# Word Embeddings in Action - Word2Vec\n",
        "\n",
        "Steps to follow:\n",
        "\n",
        "1. Get data\n",
        "2. Clean text data\n",
        "3. Tokenization\n",
        "4. Prepare vocabulary\n",
        "5. Download pre-trained embeddings\n",
        "6. Get word vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPnpxm2s7Zty"
      },
      "source": [
        "# import required libraries\n",
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7ubotsYxoTi"
      },
      "source": [
        "# 1. Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgII8L1Xd0-Q"
      },
      "source": [
        "#input text\n",
        "text=['Building some bots for Wikipedia.',\n",
        "      'Wikipedia is flooded with information!@#.',\n",
        "      '&*^%There is an app for everthing.']"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndLOg1M3xw5u"
      },
      "source": [
        "# 2. Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DpVJTY4uzCM"
      },
      "source": [
        "# cleaning\n",
        "import re\n",
        "\n",
        "def clean(text):\n",
        "  #lower case\n",
        "  text=text.lower()\n",
        "\n",
        "  #remove punctuations\n",
        "  text=re.sub('[^a-zA-Z]',\" \",text)\n",
        "\n",
        "  return text"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W8UmlIZu3Bq"
      },
      "source": [
        "#call the clean function\n",
        "cleaned_text=[]\n",
        "\n",
        "for i in text:\n",
        "  cleaned_text.append(clean(i))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lT9vtLAX48l",
        "outputId": "fdad6587-2e9f-4d5f-9230-c38c1c6dd892"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['building some bots for wikipedia ',\n",
              " 'wikipedia is flooded with information ',\n",
              " 'there is an app for everthing ']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSslu0ObyCxo"
      },
      "source": [
        "# 3. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF8vhjlDu4Y-",
        "outputId": "28d690e4-b0cd-46b4-f799-bb6a34c235e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#tokenize the text\n",
        "tokens=[]\n",
        "\n",
        "for i in cleaned_text:\n",
        "  tokens.append(i.split())\n",
        "\n",
        "print(tokens)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['building', 'some', 'bots', 'for', 'wikipedia'], ['wikipedia', 'is', 'flooded', 'with', 'information'], ['there', 'is', 'an', 'app', 'for', 'everthing']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBcFBdRFyGzQ"
      },
      "source": [
        "# 4. Vocabulary Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNd7ic_WvFqO",
        "outputId": "c02de20b-d97d-45ad-ae75-ada505d8950e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#construct vocabulary\n",
        "vocab=[]\n",
        "\n",
        "for i in tokens:\n",
        "  for j in i:\n",
        "    if j not in vocab:\n",
        "      vocab.append(j)\n",
        "\n",
        "#remove duplicate token\n",
        "vocab = list(set(vocab))\n",
        "\n",
        "print(vocab)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['an', 'for', 'app', 'everthing', 'some', 'information', 'building', 'wikipedia', 'flooded', 'with', 'bots', 'is', 'there']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMc9TTmSyO67"
      },
      "source": [
        "#5. Feature Representation (word2vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Fpt6-ned1AI"
      },
      "source": [
        "### Download Google's pre-trained Word2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQNZ7Exh3jky",
        "outputId": "189c6a66-62d3-4c45-aae3-62211a28bf04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# download and extract word2vec embeddings\n",
        "#! wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "! wget -c \"https://radimrehurek.com/gensim/models/word2vec.html\"\n",
        "#! wget -c \"https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/view?resourcekey=0-wjGZdNAUop6WykTtMip30g\"\n",
        "#! gunzip GoogleNews-vectors-negative300.bin.gz\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-19 21:38:27--  https://radimrehurek.com/gensim/models/word2vec.html\n",
            "Resolving radimrehurek.com (radimrehurek.com)... 188.114.97.0, 188.114.96.0, 2a06:98c1:3121::, ...\n",
            "Connecting to radimrehurek.com (radimrehurek.com)|188.114.97.0|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘word2vec.html’\n",
            "\n",
            "word2vec.html           [ <=>                ] 133.55K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-06-19 21:38:27 (10.3 MB/s) - ‘word2vec.html’ saved [136752]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoHGUSuId1AO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "c47eef19-0e5c-4374-f5db-4d7ef049a8dd"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# path of the downloaded model\n",
        "#filename = 'GoogleNews-vectors-negative300.bin'\n",
        "filename = 'word2vec.html'\n",
        "\n",
        "# load into gensim\n",
        "w2vec = KeyedVectors.load_word2vec_format(filename, binary=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c98e5f058dc4>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# load into gensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mw2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m             \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5WyFsVHd1Aj"
      },
      "source": [
        "Once you have executed the above code, your word2vec embeddings are finally installed and loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assnsO6Ld1Ap"
      },
      "source": [
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Please note that the length of every vector of the pre-trained word2vec embeddings is 300.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV0yy-P0sPKf"
      },
      "source": [
        "# empty array of shape (no. of tokens X 300) to store word2vec features\n",
        "wordvec_array = np.zeros((len(vocab), 300))\n",
        "\n",
        "for i,j in enumerate(vocab):\n",
        "  wordvec_array[i,:] = w2vec.wv.word_vec(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpsWAoaap5AA",
        "outputId": "d8a0dab2-8b49-4941-c3e2-238f40c8cdb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "wordvec_array"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.12597656,  0.19042969,  0.06982422, ...,  0.0612793 ,\n",
              "         0.17285156, -0.07861328],\n",
              "       [ 0.11572266, -0.29101562, -0.30664062, ..., -0.24609375,\n",
              "        -0.17773438,  0.16113281],\n",
              "       [ 0.41992188,  0.12011719, -0.06787109, ...,  0.00836182,\n",
              "        -0.25976562,  0.0279541 ],\n",
              "       ...,\n",
              "       [ 0.09423828, -0.02282715,  0.05224609, ..., -0.046875  ,\n",
              "         0.16113281, -0.19921875],\n",
              "       [ 0.21875   , -0.12207031, -0.00296021, ..., -0.35351562,\n",
              "        -0.25195312, -0.11621094],\n",
              "       [-0.02490234,  0.02197266, -0.03540039, ...,  0.01080322,\n",
              "        -0.01879883, -0.06884766]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRbv0MVl_hG1"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}